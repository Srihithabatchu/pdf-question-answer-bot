{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srihithabatchu/pdf-question-answer-bot/blob/main/Zidio_project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transfer Learning using ResNet50 ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load dataset (CIFAR-10 for simplicity)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = tf.image.resize(x_train, (224, 224))\n",
        "x_test = tf.image.resize(x_test, (224, 224))\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Base Model (pretrained)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # freeze layers\n",
        "\n",
        "# Add custom layers\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)\n",
        "\n",
        "# Fine-tuning\n",
        "base_model.trainable = True\n",
        "model.compile(optimizer=Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_ft = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "mVEwsA7Z3kVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Autoencoder for Image Denoising ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Add Gaussian noise\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(size=x_test.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Build Autoencoder\n",
        "input_img = layers.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2,2))(x)\n",
        "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2,2))(x)\n",
        "decoded = layers.Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=10,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# Predictions\n",
        "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Visualize\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(3, n, i+1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.axis('off')\n",
        "    if i == 0: plt.ylabel(\"Original\")\n",
        "\n",
        "    ax = plt.subplot(3, n, i+1+n)\n",
        "    plt.imshow(x_test_noisy[i])\n",
        "    plt.axis('off')\n",
        "    if i == 0: plt.ylabel(\"Noisy\")\n",
        "\n",
        "    ax = plt.subplot(3, n, i+1+2*n)\n",
        "    plt.imshow(decoded_imgs[i])\n",
        "    plt.axis('off')\n",
        "    if i == 0: plt.ylabel(\"Denoised\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KLgXGbtO3zmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RNN for Twitter US Airline Sentiment ---\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load dataset\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    \"airline_sentiment.csv\",\n",
        "    \"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\n",
        ")\n",
        "import pandas as pd\n",
        "data = pd.read_csv(dataset)\n",
        "data = data[['text', 'airline_sentiment']]\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "data['label'] = le.fit_transform(data['airline_sentiment'])\n",
        "\n",
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['text'].values, data['label'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "max_tokens = 10000\n",
        "max_len = 100\n",
        "vectorizer = layers.TextVectorization(max_tokens=max_tokens, output_sequence_length=max_len)\n",
        "vectorizer.adapt(train_texts)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels)).batch(32).map(lambda x, y: (vectorizer(x), y))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels)).batch(32).map(lambda x, y: (vectorizer(x), y))\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(max_tokens, 128),\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "    layers.Bidirectional(layers.LSTM(32)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_ds, validation_data=test_ds, epochs=5)\n",
        "\n",
        "# Plot accuracy/loss\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='val')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Test on custom tweets\n",
        "sample_tweets = [\"I love flying with Delta!\", \"Worst flight ever.\", \"It was okay, nothing special.\"]\n",
        "preds = model.predict(vectorizer(sample_tweets))\n",
        "for t, p in zip(sample_tweets, preds):\n",
        "    print(f\"{t} â†’ {le.inverse_transform([p.argmax()])[0]}\")\n"
      ],
      "metadata": {
        "id": "oAscTR50320R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transfer Learning for Plant Disease Classification ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Use TensorFlow Flowers dataset as plant proxy\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "\n",
        "# Image generator\n",
        "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
        "train_gen = datagen.flow_from_directory(data_dir, target_size=(224,224), subset='training', batch_size=32)\n",
        "val_gen = datagen.flow_from_directory(data_dir, target_size=(224,224), subset='validation', batch_size=32)\n",
        "\n",
        "# Pre-trained model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
        "\n",
        "# Fine-tuning\n",
        "base_model.trainable = True\n",
        "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_ft = model.fit(train_gen, validation_data=val_gen, epochs=3)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(val_gen)\n",
        "print(f\"Validation Accuracy after fine-tuning: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "X-vGasvF37PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMHqVQ4y4Mvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}